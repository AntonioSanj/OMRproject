{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15f40cdc-f10e-4e65-b906-60ed537bd306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:27:47.722082900Z",
     "start_time": "2024-12-22T19:27:47.710082300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3710a6aa-935f-40f7-b883-fb78dbeaf14f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:27:48.563940100Z",
     "start_time": "2024-12-22T19:27:48.545941200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "class CocoTransform:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)  # Convert PIL image to tensor\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de00340b-2882-46c2-8090-a4e4d70c9dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:27:50.321944100Z",
     "start_time": "2024-12-22T19:27:50.259678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from constants import myDataImg, myDataCoco\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "def get_coco_dataset(img_dir, ann_file):\n",
    "    return CocoDetection(\n",
    "        root=img_dir,\n",
    "        annFile=ann_file,\n",
    "        transforms=CocoTransform()\n",
    "    )\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = get_coco_dataset(\n",
    "    img_dir=myDataImg,\n",
    "    ann_file=myDataCoco\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = get_coco_dataset(\n",
    "    img_dir=myDataImg,\n",
    "    ann_file=myDataCoco\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b9f141a-dc16-46cd-b735-38b3241b3062",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:27:51.538671600Z",
     "start_time": "2024-12-22T19:27:51.530673700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Faster R-CNN with ResNet-50 backbone\n",
    "def get_model(num_classes):\n",
    "    # Load pre-trained Faster R-CNN\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8b8f9e2-e284-490b-9d0c-fe9ef564e7ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:27:53.307504600Z",
     "start_time": "2024-12-22T19:27:52.639523600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "num_classes = 11 # Background + categories\n",
    "model = get_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64b71051-288b-455c-85a5-8c8454390488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:27:54.485922500Z",
     "start_time": "2024-12-22T19:27:54.236924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc561163-c6e9-4899-9f4a-a6a8702364f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:27:56.186375500Z",
     "start_time": "2024-12-22T19:27:56.175374800Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    for images, targets in data_loader:\n",
    "        # Move images to the device\n",
    "        images = [img.to(device) for img in images]\n",
    "\n",
    "        # Validate and process targets\n",
    "        processed_targets = []\n",
    "        valid_images = []\n",
    "        for i, target in enumerate(targets):\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            for obj in target:\n",
    "                # Extract bbox\n",
    "                bbox = obj[\"bbox\"]  # Format: [x, y, width, height]\n",
    "                x, y, w, h = bbox\n",
    "\n",
    "                # Ensure the width and height are positive\n",
    "                if w > 0 and h > 0:\n",
    "                    boxes.append([x, y, x + w, y + h])  # Convert to [x_min, y_min, x_max, y_max]\n",
    "                    labels.append(obj[\"category_id\"])\n",
    "\n",
    "            # Only process if there are valid boxes\n",
    "            if boxes:\n",
    "                processed_target = {\n",
    "                    \"boxes\": torch.tensor(boxes, dtype=torch.float32).to(device),\n",
    "                    \"labels\": torch.tensor(labels, dtype=torch.int64).to(device),\n",
    "                }\n",
    "                processed_targets.append(processed_target)\n",
    "                valid_images.append(images[i])  # Add only valid images\n",
    "\n",
    "        # Skip iteration if no valid targets\n",
    "        if not processed_targets:\n",
    "            continue\n",
    "\n",
    "        # Ensure images and targets are aligned\n",
    "        images = valid_images\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, processed_targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch}] Loss: {losses.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fc20ce0-c7e1-485f-bd54-09da71fa20b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:31:31.218565500Z",
     "start_time": "2024-12-22T19:27:57.618030100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] Loss: 1.9997\n",
      "Model saved: fasterrcnn_resnet50_epoch_1.pth\n",
      "Epoch [1] Loss: 1.5720\n",
      "Model saved: fasterrcnn_resnet50_epoch_2.pth\n",
      "Epoch [2] Loss: 1.2752\n",
      "Model saved: fasterrcnn_resnet50_epoch_3.pth\n",
      "Epoch [3] Loss: 1.4680\n",
      "Model saved: fasterrcnn_resnet50_epoch_4.pth\n",
      "Epoch [4] Loss: 1.3442\n",
      "Model saved: fasterrcnn_resnet50_epoch_5.pth\n",
      "Epoch [5] Loss: 1.1699\n",
      "Model saved: fasterrcnn_resnet50_epoch_6.pth\n",
      "Epoch [6] Loss: 1.4439\n",
      "Model saved: fasterrcnn_resnet50_epoch_7.pth\n",
      "Epoch [7] Loss: 1.3209\n",
      "Model saved: fasterrcnn_resnet50_epoch_8.pth\n",
      "Epoch [8] Loss: 1.2814\n",
      "Model saved: fasterrcnn_resnet50_epoch_9.pth\n",
      "Epoch [9] Loss: 1.1796\n",
      "Model saved: fasterrcnn_resnet50_epoch_10.pth\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save the model's state dictionary after every epoch\n",
    "    model_path = f'fasterrcnn_resnet50_epoch_{epoch + 1}.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3335f-1e27-472b-aa78-bb8006abab92",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-22T17:59:10.067986100Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02867f6c-8eda-4cd5-b572-74fcd435cf10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:33:45.931308700Z",
     "start_time": "2024-12-22T19:33:44.424454800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 264.5876  1120.666    296.2124  1173.7136 ]\n",
      " [1572.5623  1097.4103  1604.7489  1153.473  ]\n",
      " [ 454.69122 2401.9507   487.28732 2462.4453 ]\n",
      " [ 731.6745   532.9377   763.7731   586.9081 ]\n",
      " [1725.0447  1837.9211  1759.5242  1891.8783 ]\n",
      " [1046.2917  1097.7294  1078.8959  1161.261  ]\n",
      " [1078.8228  1845.7861  1112.5675  1896.9641 ]\n",
      " [1379.9502  1795.1554  1407.7598  1853.2228 ]\n",
      " [1221.3679  2410.2673  1255.364   2476.311  ]\n",
      " [ 628.2874  1103.2101   662.0002  1155.8424 ]\n",
      " [ 819.8969  2261.9695   854.03235 2325.0022 ]\n",
      " [ 911.1759  1539.2697   944.2276  1597.1064 ]\n",
      " [1679.4602  1096.8401  1710.0789  1153.3114 ]\n",
      " [ 274.53766 1392.0135   308.4154  1452.721  ]\n",
      " [1169.1348  1096.5345  1199.7454  1153.4275 ]\n",
      " [ 922.254   2276.6165   956.28107 2332.5605 ]\n",
      " [ 643.8166  1964.3503   678.13855 2026.2722 ]\n",
      " [ 263.6178  1964.0961   297.3778  2014.8661 ]\n",
      " [ 338.368   1389.2574   367.10544 1448.1075 ]\n",
      " [ 274.60092 1820.6672   308.09164 1886.0742 ]\n",
      " [ 929.4612  1811.991    960.0582  1872.8049 ]\n",
      " [ 775.66425 1542.4172   810.3017  1597.3047 ]\n",
      " [1575.3038  1381.4116  1606.2175  1435.5447 ]\n",
      " [ 769.957   1958.6378   807.93964 2015.6276 ]\n",
      " [ 382.1735  1109.5667   415.63565 1171.308  ]\n",
      " [1047.8109  1516.5928  1079.9553  1571.1625 ]\n",
      " [1132.0641  1991.5896  1166.7526  2051.7312 ]\n",
      " [ 395.95117 1377.6019   430.51135 1439.1318 ]\n",
      " [ 264.09454 1524.4838   297.34625 1580.0931 ]\n",
      " [ 387.48483 1526.5122   420.90384 1584.711  ]\n",
      " [1652.267   1991.446   1685.2651  2049.1895 ]\n",
      " [ 544.4405  1965.8003   576.52484 2018.0519 ]\n",
      " [1022.9899  1997.387   1057.2656  2050.7834 ]\n",
      " [1683.4106  1530.6298  1718.2483  1592.1217 ]\n",
      " [1022.89355 1830.9769  1057.435   1896.0388 ]\n",
      " [1637.6575  1391.1537  1665.5758  1459.7712 ]\n",
      " [ 523.8991  1530.596    559.4034  1592.3823 ]\n",
      " [ 821.2994  2403.7097   854.76105 2465.237  ]\n",
      " [1559.5989  1528.3704  1594.208   1591.8986 ]\n",
      " [ 506.86127 1102.5381   540.34467 1167.287  ]\n",
      " [ 923.1407  1371.5687   955.58093 1429.3052 ]\n",
      " [ 925.2411  1094.1991   958.70483 1150.5392 ]\n",
      " [1446.6063  1091.8417  1480.1558  1149.4952 ]\n",
      " [1436.6571  1527.4965  1471.7495  1583.1506 ]\n",
      " [ 792.42444  958.19116  821.84595 1019.0705 ]\n",
      " [ 412.456   1962.7083   448.18408 2017.666  ]\n",
      " [1295.7437  1090.3549  1329.479   1148.2375 ]\n",
      " [ 649.36346 1406.3817   682.7703  1467.7528 ]\n",
      " [ 463.1935  2231.3926   497.84695 2288.1729 ]\n",
      " [1157.6324  1570.0162  1190.1621  1621.3717 ]\n",
      " [ 790.0394  1824.9421   819.7442  1892.413  ]\n",
      " [ 651.8178  1543.4069   683.74884 1596.693  ]\n",
      " [1295.6769  1522.7881  1329.0254  1580.6228 ]\n",
      " [1439.18    1804.8322  1468.6019  1861.477  ]\n",
      " [1497.0028  1806.677   1531.0696  1875.0879 ]\n",
      " [1515.8468  1372.6659  1544.9641  1432.2216 ]\n",
      " [ 487.4158  1804.0347   520.7959  1865.0369 ]\n",
      " [1560.8082  1803.801   1593.0317  1866.0726 ]\n",
      " [ 779.84106 1089.661    813.7971  1147.6525 ]\n",
      " [ 994.8002   937.86224 1028.7806   995.6305 ]\n",
      " [ 668.8418  2229.129    701.22046 2297.829  ]\n",
      " [ 554.3918  1811.2908   585.2423  1871.7798 ]\n",
      " [1230.1261  2240.635   1266.6396  2304.8953 ]\n",
      " [ 979.9505  1831.3304  1012.56744 1893.1927 ]\n",
      " [ 608.16504  685.7554   644.90424  745.15137]\n",
      " [ 413.13522  665.9324   449.0154   735.4657 ]\n",
      " [ 912.7268  1965.2247   948.8663  2015.5322 ]\n",
      " [1046.1702  2408.0232  1083.7681  2471.0317 ]\n",
      " [1518.4872   940.5367  1552.1003  1000.3485 ]\n",
      " [1049.7062  2276.2346  1084.2122  2328.9087 ]\n",
      " [1449.2954  2254.6624  1481.6616  2322.5586 ]\n",
      " [ 621.71515  503.72507  652.448    567.7716 ]\n",
      " [1117.1661   964.42847 1150.7257  1029.7534 ]\n",
      " [ 653.513   2397.091    689.0211  2468.6155 ]\n",
      " [1341.0277  2256.024   1370.9104  2323.1636 ]\n",
      " [1051.6912   685.0856  1086.5142   755.6225 ]\n",
      " [ 360.51828  521.2841   393.71298  579.6043 ]\n",
      " [ 562.59576 2240.3203   596.88586 2300.7917 ]\n",
      " [1438.8273  1805.2977  1469.0259  1860.557  ]\n",
      " [1220.6617  1829.7577  1250.6097  1888.5476 ]\n",
      " [1308.6888  1794.6937  1346.5886  1854.4854 ]\n",
      " [ 922.4272  1372.1548   955.6635  1429.3615 ]\n",
      " [ 175.35779 2232.539    208.90569 2294.4653 ]\n",
      " [1637.513   1391.7169  1665.8373  1460.5735 ]\n",
      " [ 175.07532 1365.3337   209.91402 1432.062  ]\n",
      " [1229.466   2241.3486  1266.656   2305.0386 ]\n",
      " [1194.6548   506.25305 1229.22     580.9506 ]\n",
      " [ 288.18427 2260.6543   325.06244 2322.0322 ]\n",
      " [ 175.18172  929.7948   210.27202  999.17444]\n",
      " [ 362.84427  676.6517   394.0356   730.0948 ]\n",
      " [ 284.77066  496.90826  318.03934  568.4942 ]\n",
      " [1496.4031  1808.0273  1531.0981  1875.1769 ]\n",
      " [ 395.2063  1377.618    430.40714 1439.8059 ]\n",
      " [ 462.80347 2231.5476   498.06802 2288.6392 ]\n",
      " [1647.6947  1986.2943  1684.1315  2047.1403 ]\n",
      " [ 486.70206 1803.9972   521.284   1864.9392 ]\n",
      " [1380.1158  1794.9181  1408.6124  1853.3909 ]\n",
      " [1515.5621  1372.1746  1545.4244  1432.5986 ]\n",
      " [1194.1943   508.53738 1229.5625   579.34735]\n",
      " [ 174.04648 1793.9913   212.13222 1872.6736 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x1000 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from constants import *\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load Faster R-CNN with ResNet-50 backbone\n",
    "def get_model(num_classes):\n",
    "    # Load pre-trained Faster R-CNN\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 11  # Background + chair + person + table\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "model = get_model(num_classes)\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\Usuario\\Desktop\\UDC\\QUINTO\\TFG\\src_code\\learning\\FasterRCNN\\fasterrcnn_resnet50_epoch_10.pth'))\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Open image\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0)  # Convert image to tensor and add batch dimension\n",
    "    return image_tensor.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Load the unseen image\n",
    "image_path = feelTheLove\n",
    "image_tensor = prepare_image(image_path)\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    prediction = model(image_tensor)\n",
    "\n",
    "# `prediction` contains:\n",
    "# - boxes: predicted bounding boxes\n",
    "# - labels: predicted class labels\n",
    "# - scores: predicted scores for each box (confidence level)\n",
    "COCO_CLASSES = {0: \"Background\", 1: \"One\", 2: \"Double\", 3: \"Four\", 4: \"Half\", 5: \"Quarter\", 6: \"GClef\", 7: \"FClef\", 8: \"OpeningBracket\", 9: \"RestOne\", 10: \"RestHalf\"}\n",
    "\n",
    "def get_class_name(class_id):\n",
    "    return COCO_CLASSES.get(class_id, \"Unknown\")\n",
    "    \n",
    "# Draw bounding boxes with the correct class names and increase image size\n",
    "def draw_boxes(image, prediction, fig_size=(10, 10)):\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()  # Get predicted bounding boxes\n",
    "    print(boxes)\n",
    "    labels = prediction[0]['labels'].cpu().numpy()  # Get predicted labels\n",
    "    scores = prediction[0]['scores'].cpu().numpy()  # Get predicted scores\n",
    "    \n",
    "    # Set a threshold for showing boxes (e.g., score > 0.5)\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # Set up the figure size to control the image size\n",
    "    plt.figure(figsize=fig_size)  # Adjust the figure size here\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > threshold:\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            class_name = get_class_name(label)  # Get the class name\n",
    "            plt.imshow(image)  # Display the image\n",
    "            plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                                              linewidth=2, edgecolor='r', facecolor='none'))\n",
    "            plt.text(x_min, y_min, f\"{class_name} ({score:.2f})\", color='r')\n",
    "    \n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "\n",
    "# Display the image with bounding boxes and correct labels\n",
    "draw_boxes(Image.open(image_path), prediction, fig_size=(12, 10))  # Example of increased size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "231a65af-e139-4f49-b252-077473fb6a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T18:38:36.578077Z",
     "start_time": "2024-12-22T18:38:36.529401500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7a2f2e818b4741e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
